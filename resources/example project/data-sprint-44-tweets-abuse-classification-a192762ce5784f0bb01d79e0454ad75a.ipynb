{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDisalNlaYQs"
   },
   "source": [
    "![dphi banner](https://dphi-courses.s3.ap-south-1.amazonaws.com/Datathons/dphi_banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq0W0R_SWun_"
   },
   "source": [
    "# **Getting Started Code For [Data Sprint #44](https://dphi.tech/challenges/data-sprint-44-tweets-abuse-classification/152/overview/about) on DPhi**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCfCO8Z90f0c"
   },
   "source": [
    "# Loading Libraries\n",
    "All Python capabilities are not loaded into our working environment by default (even those that are already installed in your system). So, we import each and every library that we want to use.\n",
    "\n",
    "In data science, numpy and pandas are the most commonly used libraries. Numpy is required for calculations like mean, median, square roots, etc. Pandas is used for data processing and data frames. We choose alias names for our libraries for the sake of our convenience (numpy --> np and pandas --> pd).\n",
    "\n",
    "Note: You can import all the libraries that you think will be required or can import it as you go along. \n",
    "\n",
    "Here we are importing two libraries - numpy and pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AdgJJlH40_bQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mkuma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mkuma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mkuma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np        # Fundamental package for linear algebra and multidimensional arrays\n",
    "import pandas as pd       # Data analysis and manipulation tool\n",
    "\n",
    "# to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1NzvInd1kx6"
   },
   "source": [
    "# Loading Dataset\n",
    "Pandas module is used for reading files.\n",
    "\n",
    "You can learn more about pandas [here](https://dphi.tech/learn/introduction-to-pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5inGNMiR1hKx"
   },
   "outputs": [],
   "source": [
    "# In read_csv() function, we have passed the location to where the files are located in the dphi official github page.\n",
    "train_data = pd.read_csv(\"train_set_label.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ndV57522Czi"
   },
   "source": [
    "## What do you need to do now?\n",
    "*  Perform EDA and Data Visualization, to understand the data. Learn more about EDA [here](https://dphi.tech/learn/introduction-to-exploratory-data-analysis). Learn more about data visualization [here](https://dphi.tech/learn/introduction-to-data-visualization)\n",
    "*  Clean the data if required (like removing or filling missing values, treat outliers, etc.). Learn more about handling missing values [here](https://youtu.be/EaGbS7eWSs0)\n",
    "*  Perform Data Preprocessing if you feel it's required. Learn one hot encoding [here](https://youtu.be/9yl6-HEY7_s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIDMrCje8oXu"
   },
   "source": [
    "## Basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "id": "bp6SaZm68qz-",
    "outputId": "70616d41-9f96-436b-f2bb-6c120a7232f6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>start your day w your daily here</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>even a perfect life doesn’t feel perfect in so...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>great ready for next week s q amp a with white...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5 ways to protect your mental health during th...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i m officially an occupational therapist passe...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label\n",
       "0                   start your day w your daily here      3\n",
       "1  even a perfect life doesn’t feel perfect in so...      3\n",
       "2  great ready for next week s q amp a with white...      3\n",
       "3  5 ways to protect your mental health during th...      3\n",
       "4  i m officially an occupational therapist passe...      3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l0VymVqb8vam",
    "outputId": "a1611319-ab64-49cd-a649-15b849e4bee4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2350 entries, 0 to 2349\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tweet   2349 non-null   object\n",
      " 1   label   2350 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 36.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['tweet'] = train_data['tweet'].str.lower()\n",
    "\n",
    "\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', str(text))\n",
    "    return text\n",
    "\n",
    "train_data['tweet'] = train_data['tweet'].apply(remove_special_characters, remove_digits=False)\n",
    "\n",
    "def f(r):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    words = nltk.word_tokenize(r)\n",
    "    lemmatized_words = [wnl.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "train_data['tweet'] = train_data['tweet'].apply(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "data_X = cv.fit_transform(train_data['tweet']).toarray()\n",
    "data_X = pd.DataFrame(data_X, columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJZTl-Hn6SqG"
   },
   "source": [
    "# Separating Input Features and Output Features\n",
    "Before building any machine learning model, we always separate the input variables and output variables. Input variables are those quantities whose values are changed naturally in an experiment, whereas output variable is the one whose values are dependent on the input variables. So, input variables are also known as independent variables as its values are not dependent on any other quantity, and output variable/s are also known as dependent variables as its values are dependent on other variable i.e. input variables.\n",
    "By convention input variables are represented with 'X' and output variables are represented with 'y'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bG7oHnzB1_NJ"
   },
   "outputs": [],
   "source": [
    "X = data_X\n",
    "y = train_data.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydj91s_E64Uu"
   },
   "source": [
    "# Splitting the data into Train and Validation Set\n",
    "We want to check the performance of the model that we built. For this purpose, we always split (both input and output data) the given data into training set which will be used to train the model, and test set which will be used to check how accurately the model is predicting outcomes.\n",
    "\n",
    "For this purpose we have a class called 'train_test_split' in the 'sklearn.model_selection' module.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "m7EGP5w360BD"
   },
   "outputs": [],
   "source": [
    "# import train_test_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Dj5_kJ2Y7FnS"
   },
   "outputs": [],
   "source": [
    "# split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsxciXev7Q_i"
   },
   "source": [
    "# Building Model\n",
    "Now we are finally ready, and we can train the model.\n",
    "\n",
    "There are tons of Machine Learning models like Logistic Regression, Random Forest, Decision Tree, etc. to say you some. However here we are using Logistic Regression (again, using the sklearn library).\n",
    "\n",
    "Then we would feed the model both with the data (X_train) and the answers for that data (y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ay4pnyq87LO8"
   },
   "outputs": [],
   "source": [
    "# Importing LogisticRegression from sklearn.linear_model\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sE6vHN6t763C"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e23pEgaC8G8l"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4YmXvx3B7-FH",
    "outputId": "a832dc72-d7e0-4dad-e5a7-70ecc72de13e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKO513vA8Vhu"
   },
   "source": [
    "# Validate The Model\n",
    "Wonder🤔 how well your model learned! Lets check it.\n",
    "\n",
    "### Predict on the validation data (X_val)\n",
    "Now we predict using our trained model on the validation set we created i.e. X_val and evaluate our model on unforeseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0d5YdS-W8RD1"
   },
   "outputs": [],
   "source": [
    "pred = lr.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYD_9rcY9xkl"
   },
   "source": [
    "## Model Evaluation\n",
    "Evaluating performance of the machine learning model that we have built is an essential part of any machine learning project. Performance of our model is done using some evaluation metrics.\n",
    "\n",
    "There are so many evaluation metrics to use for regression problem, naming some - Accuracy Score, F1 Score, Precision, Recall etc. However, **Accuracy Score** is the metric for this challenge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ce4XV0GS-u_r"
   },
   "outputs": [],
   "source": [
    "# import mean squared error from sklearn.metric\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LBI7JwY1-40F",
    "outputId": "ece185c1-87a7-4be4-e008-9980847fa88c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score is:  0.8906562589763701\n"
     ]
    }
   ],
   "source": [
    "print('F1 Score is: ', f1_score(y_val, pred, average='weighted')) \n",
    "\n",
    "# y_val is the original target value of the validation set (X_val)\n",
    "# pred is the predicted target value of the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-b1C_HVx_lKV"
   },
   "source": [
    "# Predict The Output For Testing Dataset 😅\n",
    "We have trained our model, evaluated it and now finally we will predict the output/target for the testing data (i.e. testing_set_label.csv) given in 'Data' section of the problem page.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BlJRzftAjTO"
   },
   "source": [
    "## Load Test Set\n",
    "Load the test data on which final submission is to be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "N_LHXg8ZAilM"
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_set_label.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSlXRaJRA2K9"
   },
   "source": [
    "**Note:** \n",
    "*  Use the same techniques to deal with missing values as done with the training dataset.   \n",
    "\n",
    "*  **Don't remove any observation/record from the test dataset otherwise you will get wrong answer. The number of items in your prediction should be same as the number of records are present in the test dataset**.\n",
    "\n",
    "*  Use the same techniques to preprocess the data as done with training dataset.\n",
    "\n",
    "***Why do we need to do the same procedure of filling missing values, data cleaning and data preprocessing on the new test data as it was done for the training and validation data?***\n",
    "\n",
    "**Ans:** Because our model has been trained on certain format of data and if we don't provide the testing data of the similar format, the model will give erroneous predictions and the rmse of the model will increase. Also, if the model was build on 'n' number of features, while predicting on new test data you should always give the same number of features to the model. In this case if you provide different number of features while predicting the output, your ML model will throw a ValueError saying something like 'number of features given x; expecting n'. Not confident about these statements? Well, as a data scientist you should always perform some experiment and observe the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dPUaiH-Zyhj_",
    "outputId": "c8842772-e40c-47b3-c4c4-53a3977907ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 784 entries, 0 to 783\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tweet   784 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 6.2+ KB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['tweet'] = test_data['tweet'].str.lower()\n",
    "\n",
    "\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', str(text))\n",
    "    return text\n",
    "\n",
    "test_data['tweet'] = test_data['tweet'].apply(remove_special_characters, remove_digits=False)\n",
    "\n",
    "def f(r):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    words = nltk.word_tokenize(r)\n",
    "    lemmatized_words = [wnl.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "test_data['tweet'] = test_data['tweet'].apply(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = cv.transform(test_data['tweet']).toarray()\n",
    "test_data = pd.DataFrame(test, columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1VWI-oiCp3t"
   },
   "source": [
    "## Make Prediction on Test Dataset\n",
    "Time to make submission!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "i3Fsfy9iA1N7"
   },
   "outputs": [],
   "source": [
    "target = lr.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrNaxq9KEp0n"
   },
   "source": [
    "#### Note: **Follow the submission guidelines given in 'How To Submit' Section.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iH2U3r7CC99s"
   },
   "source": [
    "## How to save prediciton results locally via jupyter notebook?\n",
    "If you are working on Jupyter notebook, execute below block of codes. A file named 'prediction_results.csv' will be created in your current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "jyT2vAGIDjnF"
   },
   "outputs": [],
   "source": [
    "res = pd.DataFrame(target) #target is nothing but the final predictions of your model on input features of your new unseen test data\n",
    "res.columns = [\"prediction\"]\n",
    "res.to_csv(\"submission.csv\", index = False)      # the csv file will be saved locally on the same location where this notebook is located."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98_QXo0TD_0G"
   },
   "source": [
    "# **OR**, \n",
    "**if you are working on Google Colab then use the below set of code to save prediction results locally**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfrE2CjoDwje"
   },
   "source": [
    "## How to save prediction results locally via colab notebook?\n",
    "If you are working on Google Colab Notebook, execute below block of codes. A file named 'prediction_results' will be downloaded in your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "JgcOUpjKDzzI",
    "outputId": "96155fe1-fecf-4d78-d229-e1e05c4d2f4d"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_e5cffe97-e465-451d-aecc-26b0e61d7b74\", \"submission.csv\", 7974)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To create Dataframe of predicted value with particular respective index\n",
    "res = pd.DataFrame(target) # target are nothing but the final predictions of your model on input features of your new unseen test data\n",
    "res.columns = [\"prediction\"]\n",
    "\n",
    "# To download the csv file locally\n",
    "from google.colab import files\n",
    "res.to_csv('submission.csv', index = False)         \n",
    "files.download('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VghEzJ8zE4kM"
   },
   "source": [
    "## **Well Done! 👍**\n",
    "You are all set to make a submission. Let's head to the [challenge page](https://dphi.tech/challenges/data-sprint-44-tweets-abuse-classification/152/overview/about) to make the submission."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data_Sprint_32_Poker_Hands.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
